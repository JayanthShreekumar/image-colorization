Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]
/opt/saturncloud/envs/ece60131_proj/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/opt/saturncloud/envs/ece60131_proj/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Loading model from: /opt/saturncloud/envs/ece60131_proj/lib/python3.12/site-packages/lpips/weights/v0.1/alex.pth
Starting Training Epoch 1
100%|██████████| 57/57 [00:51<00:00,  1.10it/s]
Epoch 1  Training Loss: 0.5400
Traceback (most recent call last):
  File "/home/jovyan/image-colorization/main.py", line 161, in <module>
    trainer.train()
  File "/home/jovyan/image-colorization/UNetVAE/train.py", line 85, in train
    val_metrics = self.validate(model, val_loader)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jovyan/image-colorization/UNetVAE/train.py", line 119, in validate
    lpips_score = self.lpips(outputs, targets).mean().item()
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jovyan/image-colorization/metrics.py", line 44, in __call__
    lp = self.model(pred_lp, tgt_lp)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/saturncloud/envs/ece60131_proj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/saturncloud/envs/ece60131_proj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/saturncloud/envs/ece60131_proj/lib/python3.12/site-packages/lpips/lpips.py", line 118, in forward
    in0_input, in1_input = (self.scaling_layer(in0), self.scaling_layer(in1)) if self.version=='0.1' else (in0, in1)
                            ^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/saturncloud/envs/ece60131_proj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/saturncloud/envs/ece60131_proj/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/saturncloud/envs/ece60131_proj/lib/python3.12/site-packages/lpips/lpips.py", line 154, in forward
    return (inp - self.shift) / self.scale
            ~~~~^~~~~~~~~~~~
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 35.50 MiB is free. Process 632524 has 2.32 GiB memory in use. Process 633881 has 2.32 GiB memory in use. Process 635088 has 2.32 GiB memory in use. Process 638197 has 2.61 GiB memory in use. Process 639201 has 2.61 GiB memory in use. Process 639433 has 2.61 GiB memory in use. Process 649070 has 6.31 GiB memory in use. Process 650877 has 6.31 GiB memory in use. Process 652047 has 6.31 GiB memory in use. Process 654624 has 4.29 GiB memory in use. Process 656017 has 4.29 GiB memory in use. Process 657253 has 4.29 GiB memory in use. Process 659624 has 4.71 GiB memory in use. Process 661714 has 4.71 GiB memory in use. Process 663448 has 4.71 GiB memory in use. Process 670599 has 6.63 GiB memory in use. Process 673246 has 6.63 GiB memory in use. Process 684367 has 1.53 GiB memory in use. Process 695083 has 1.53 GiB memory in use. Process 705916 has 1.53 GiB memory in use. Process 727550 has 532.00 MiB memory in use. Of the allocated memory 105.43 MiB is allocated by PyTorch, and 12.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
